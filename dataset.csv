Question,Answer,Marks,Diagram
"What is the structure of a Decision Tree, and how does it function?","A decision tree is a hierarchical model used for classification and regression tasks. It consists of nodes, where each internal node represents a decision based on a specific feature, branches indicate the possible outcomes, and leaf nodes represent the final prediction or class label. The tree is built by recursively splitting the dataset based on the feature that provides the maximum information gain or minimizes impurity. The decision-making process follows a top-down approach, starting from the root node and traversing through branches until a leaf node is reached. Decision trees are easy to interpret and can handle both numerical and categorical data but are prone to overfitting if not pruned properly.",5,"static/diagrams/DecisionTree.png"
Explain the architecture of an Artificial Neural Network (ANN)?,"An artificial neural network (ANN) is a computational model inspired by the human brain, consisting of multiple layers of interconnected neurons. The architecture includes an input layer, one or more hidden layers, and an output layer. Each neuron in a layer receives inputs, applies a weighted sum, passes it through an activation function (such as ReLU or Sigmoid), and sends the result to the next layer. ANNs learn by adjusting weights through backpropagation, which minimizes the error using optimization techniques like gradient descent. These networks are widely used in tasks like image recognition, speech processing, and natural language understanding due to their ability to learn complex patterns from data.",5,"static/diagrams/ann.png"
How does a Convolutional Neural Network (CNN) process visual data?,"A convolutional neural network (CNN) is specifically designed for image processing tasks. It consists of convolutional layers that apply filters to extract spatial features like edges, textures, and patterns. Pooling layers then reduce the dimensionality while preserving important information, making computations more efficient. The extracted features are passed through fully connected layers that classify the image based on learned representations. CNNs leverage weight sharing and local connectivity, making them highly effective in tasks such as image classification, object detection, and facial recognition.",4,"static/diagrams/cnn.png"
"What is the architecture of a Recurrent Neural Network (RNN), and how does it handle sequential data?","A recurrent neural network (RNN) is designed for sequential data processing, such as time series forecasting and natural language processing. Unlike traditional neural networks, RNNs have loops that allow information to persist across time steps. Each neuron receives input from both the current input data and the previous hidden state, enabling it to capture temporal dependencies. Variants like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) are used to address the vanishing gradient problem, ensuring better learning of long-term dependencies in sequences.",4,"static/diagrams/rnn.png"
How does the Bias-Variance Tradeoff impact model performance?,"The bias-variance tradeoff is a fundamental concept in machine learning that affects model generalization. A high-bias model makes strong assumptions about the data and tends to underfit, leading to poor accuracy on both training and test data. On the other hand, a high-variance model is overly complex, capturing noise along with patterns, resulting in overfitting and poor generalization. The goal is to find a balance where the model achieves low error on both training and unseen data. Techniques like cross-validation, regularization, and ensemble methods help manage this tradeoff.",4,"static/diagrams/bias.png"
"What is the Curse of Dimensionality, and how does it affect machine learning models?","The curse of dimensionality refers to the challenges that arise when dealing with high-dimensional data. As the number of features increases, the data points become sparse, making it difficult for machine learning models to find meaningful patterns. This leads to increased computational complexity, overfitting, and poor model performance. Dimensionality reduction techniques such as Principal Component Analysis (PCA) and feature selection help mitigate these issues by reducing the number of input variables while preserving essential information.",4,"static/diagrams/COD.jpg"
How does Principal Component Analysis (PCA) reduce dimensionality?,"Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while retaining as much variance as possible. It does this by identifying the principal components, which are new orthogonal axes that capture the maximum variance in the data. The first principal component captures the highest variance, followed by the second, and so on. PCA helps in visualization, noise reduction, and improving model performance by eliminating redundant features.",4,"static/diagrams/pca.png"
What is the difference between Euclidean Distance and Manhattan Distance in clustering algorithms?,"Euclidean distance and Manhattan distance are two common distance metrics used in machine learning algorithms. Euclidean distance measures the straight-line distance between two points in a multidimensional space, making it suitable for scenarios where all features contribute equally. Manhattan distance, on the other hand, calculates the distance by summing the absolute differences between coordinates along each axis, resembling city block distances. While Euclidean distance is useful for continuous data, Manhattan distance is often preferred in high-dimensional spaces where feature correlations are important.",5,"static/diagrams/manhattan.jpg"
"How do different data visualization plots (e.g., scatter plots, box plots, histograms) aid in understanding data distributions?","Data visualization techniques play a crucial role in understanding patterns, distributions, and relationships in a dataset. Scatter plots help identify correlations and outliers between two numerical variables. Box plots visualize data spread, median, and potential outliers, making them useful for comparing distributions. Histograms display the frequency of data points within specified intervals, helping in understanding data skewness and normality. These visualization tools assist in exploratory data analysis, feature selection, and anomaly detection.",5,"static/diagrams/plots.png"
Can you explain the concept of Gradient Descent and its variants with the help of diagrams?,"Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models by iteratively updating model parameters. It works by computing the gradient of the loss function and adjusting parameters in the direction that reduces the error. There are different variants of gradient descent:Batch Gradient Descent: Updates parameters after computing the gradient for the entire dataset.Stochastic Gradient Descent (SGD): Updates parameters for each individual training example, leading to faster convergence but with more noise.Mini-batch Gradient Descent: A compromise between batch and stochastic, updating parameters in small batches, balancing stability and speed.Diagrams illustrating the loss function surface and gradient descent steps help visualize how the algorithm converges to the optimal solution.",5,"static/diagrams/gradient.png"